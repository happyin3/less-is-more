# 海量评论系统

* 功能设计
* 架构设计
* 存储设计
* 可用性设计

## 前言

**架构设计重要的是对产品体系的理解**

* 不要做需求的翻译机，先理解业务背后的本子，事情的初衷。复制是没有灵魂的
* 理解背后的产品体系，搞清楚系统背后的背景，才能做出最佳的设计和抽象
    * 动机
    * 解决什么痛点
    * 给用户带来什么收益
    * 客户第一、解决客户的问题
* 架构设计最重要的就是理解整个产品体系在系统中的定位

> 技术牛逼，只是一个平面；真正牛逼的人，是一个球，要看综合能力

## 评论系统

我们往小里做就是视频评论系统，往大里做就是评论平台，可以接入各种业务形态。

* 发布评论：支持回复楼层、楼中楼
* 读取评论：按时间、热度排序
* 删除评论：用户删除、作者删除
* 管理评论：作者置顶、后台运营管理（搜索、删除、审核等）
    * 内部一些管理的需求

### 在动手设计前，反复思考，真正编码的时间只有5%

* 基础架构、对象存储的开发；设计的时间多的多，真正编码的时间很少
* 会在设计层面反复推演，写伪代码
* 想的不清不楚，就写代码，就是给自己挖坑
* 一个好的架构图，应该在信封上能画出来

## 架构设计

### 概览

![评论系统-架构](https://i.loli.net/2021/11/05/RUuDt8HA9aKGBhC.png)

### BFF: comment

* 复杂评论业务的服务编排，比如访问账号服务进行等级判定，同时需要在BFF面向移动端/WEB场景来设计API，这一层抽象把评论的本省的内容列表处理（加载、分页、排序等）进行了隔离，关注在业务平台化逻辑上。
* 请求不同的业务接口，依赖很多的服务。

### Service: comment-service

服务层，去平台业务的逻辑，专注在评论功能的API实现上，比如发布、读取、删除等，关注在稳定性、可用性上，这样让上游可以灵活组织逻辑把基础能力和业务能力剥离。

* 去掉平台的逻辑，专注于实现功能
* 提供海量数据的读写，围绕数据本身

### Job: comment-job

消息队列的最大用途是消峰处理

### Admin: comment-admin

* 管理平台，按照安全等级划分服务，尤其划分运营平台，他们会共享服务层的存储层（MySQL、Redis）
    * 运营侧有一些高危的API，这些GM能力独立出来
* 运营体系的数据大量都是检索，我们使用canal进行同步到ES中，整个数据的展示都是通过ES，再通过业务主键更新业务数据层，这样运营端的查询压力就下放给了独立的全文检索系统。

### Dependency: account-service, filter-service

整个评论服务还会依赖一些外部gRPC服务，统一的平台业务逻辑在comment BFF层收敛，这里account-service主要是账号服务，filter-service是敏感词过滤服务。

> 架构设计等同于数据设计，梳理清楚数据的走向和逻辑。尽量避免环形依赖，数据双向请求等。

### 架构设计-comment-service

![comment-service](https://i.loli.net/2021/11/06/1OBi4QZAV3CSyU6.png)

**comment-service，专注在评论数据处理**

* 我们一开始是comment-service和comment是一层，业务和功能耦合在一起，非常不利于迭代，当然在设计层面可以考虑目录结构进行拆分，但是架构层次来说，迭代隔离也是好的。

**读的核心逻辑**

* Cache-Aside模式，先读取缓存，再读取存储。早起cache rebuild是做到服务里的，对于重建逻辑，一般会使用read ahead的思路，即预读，用户访问了第一页，很有可能访问第二页，所以缓存会超前加载，避免频繁cache miss。当缓存抖动是否，特别容易引起集群thundering herd-惊群现象，大量的请求会触发cache rebuild，因为使用了预加载，容易导致服务OOM。所以我们开到回源的逻辑里，我们使用了消息队列来进行逻辑异步化，对于当前请求只返回mysql中部分数据即止。

**缓存的一致性**

* 1.0 缓存cache miss的时候，会有大量的数据从mysql读入redis，这个时候会出现性能抖动，出现OOM的问题；
* 2.0 mysql读取数据，只读前两页，而不是全量数据
* 通过kafka缓存，读请求（需要读取mysql的时候，然后做rebuild的操作）控制数据刷新的量
    * 如果前面已经存在类似的操作，就把重复的请求抛掉，类似merge request

**写的核心逻辑**

* 我们担心类似“明显出轨”等热点事件的发生，而且写和读相比较，写可以认为是透传到存储层的，系统的瓶颈往往就来自于存储层，或者有状态层
* 对于写的设计上，我们认为刚发布的评论有极短的延迟（通常小于几ms）对用户可见是可接收的，把对存储的直接冲击放到消息队列，按照消息反压的思路，即如果存储延迟升高，消息消费能力就下降，自然消息容易堆积，系统始终以最大化方式消费
* Kafka是存在partition概念的，可以认为是物理上的一个小队列，一个topic是由一组partition组成的，所以kafka的吞吐模型理解为：全局并行，局部串行的生产消费方式。对于入队的消息，可以按照哈希的方式进行分发。那么某个partition中的评论主题的数据一定都在一起，这样方便我们串行消费

### 架构设计-comment-admin

![comment-admin](https://i.loli.net/2021/11/06/AzoWngP7aFcBwsO.png)

* mysql binlog中的数据被canal中间件流式消费，获取到业务的原始CURD操作，需要回放录入到es中，但是es中的数据最终是面向运营体系提供服务能力，需要检索的数据维度比较多，在入es前需要做一个异构的joiner，把单表变宽预处理好join逻辑，然后倒入到es中。
* 一般来说，运营后台的检索条件都是组合的，使用es的好处是避免依赖，mysql来做多条件组合检索，同时mysql毕竟是oltp面向线上联机事务处理的。通过冗余数据的方式，使用其他引擎来实现。
* es一般会存储检索、展示、primary key等数据，当我们操作编辑的时候，找到记录的primary key，最后交由comment-admin进行运营侧的CRUD操作。

### 架构设计-comment

![comment](https://i.loli.net/2021/11/06/WE2IqtBXiwfdKuU.png)

**comment作为BFF，是面向端，面向平台，面向业务组合的服务。所以平台扩展的能力，我们都在comment服务来实现，方便统一和准入平台，以统一的接口形式提供平台化的能力。**

* 依赖其他gRPC服务，整合统一平台侧的逻辑（比如发布评论用户等级限定）
* 直接向端上提供接口，提供数据的读写接口，甚至可以整合端上，提供统一的端上SDK
* 需要对非核心依赖的gRPC服务进行降级，当这些服务不稳定时

## 存储设计

### 数据库设计

**comment_subject_[0-49]**

字段 | 类型 | 备注
---------|----------|---------
 id | int64 | 主键
 obj_id | int64 | 对象id
 obj_type | int8 | 对象类型
 member_id | int64 | 作者用户id
 count | int32 | 评论总数
 root_count | int32 | 根评论总数
 all_count | int32 | 评论+回复总数
 state | int8 | 状态（0-正常、1-隐藏）
 attrs | int32 | 属性（bit 0-运营置顶、1-up置顶、2-大数据过滤）
 create_time | timestamp | 创建时间
 update_time | timestamp | 修改时间

**comment_index_[0-199]**

字段 | 类型 | 备注
---------|----------|---------
 id | int64 | 主键
 obj_id | int64 | 对象id
 obj_type | int8 | 对象类型
 member_id | int64 | 发表者用户id
 root | int64 | 根评论ID，不为0是回复评论
 parent | int64 | 父评论ID，为0是root评论
 floor | int32 | 评论楼层
 count | int32 | 评论总数
 root_count | int32 | 根评论总数
 like | int32 | 点赞数
 hate | int32 | 点踩数
 state | int8 | 状态（0-正常、1-隐藏）
 attrs | int32 | 属性
 create_time | timestamp | 创建时间
 update_time | timestamp | 修改时间

**comment_content_[0-199]**

字段 | 类型 | 备注
---------|----------|---------
 comment_id | int64 | 主键
 at_member_ids | string | 对象id
 ip | int64 | 对象类型
 platform | int8 | 发表者用户id
 device | string | 根评论ID，不为0是回复评论
 message | string | 评论内容
 meta | string | 评论元数据：背景、字体
 create_time | timestamp | 创建时间
 update_time | timestamp | 修改时间

**数据写入**

事务更新comment_subject、comment_index、comment_content三张表，其中content属于非强制需要一致性考虑的。可以先写入content，之后事务更新其他表。即便content先成功，后续失败仅仅存储一条脏数据。

**数据读取**

基于obj_id + obj_type在comment_index表找到评论列表。之后根据comment_index的id字段找到comment_content的评论内容。对于二级的子楼层。。。

### 索引内容分离

**comment_index：评论楼层的索引组织表，实际并不包含内容**

**comment_content：评论内容的表，包含评论的具体内容。其中comment_index的id字段和comment_content是1对1的关系，这里面包含几种设计思想。**

* 表都有主键，即cluster index，是物理阻止形式存放的，comment_content没有id，是为了减少一次二级索引查找，直接基于主键检索，同时comment_id在写入要尽可能的顺序自增。
* 索引、内容分离，方便mysql datapage缓存更多的row，如果和content耦合，会导致更大的IO。长远来看content信息可以直接使用KV storage存储。

### 缓存设计

**comment_subject_cache[string]**

key | string | oid_type
---------|----------|---------
 value | int64 | subject marshal string
 expire | duration | 24h

对应主题的缓存，value使用protobuf序列化的方式存入。我们早起使用memcache来进行缓存，因为redis早期单线程模型，吞吐能力不高。

**comment_index_cache[sorted set]**

key | string | cache key: oid_type_sort，其中sort为排序方式，0：楼层，1：回复数量
---------|----------|---------
 member | int64 | comment_id：评论ID
 score | double | 楼层号、回复数量、排序得分
 expire | duration | 8h

使用redis sortedset进行索引的缓存，索引即数据的组织顺序，而非数据内容。

**comment_content_cache[string]**

key | string | comment_id
---------|----------|---------
 value | int64 | content marshal string
 expire | duration | 24h

对应评论内容数据，使用protobuf序列化的方式存入。

## 可用性设计

### Singleflight

对于热门的主题，如果存在缓存穿透的情况，会导致大量的同进程、跨进程的数据回源到存储层，可能会引起存储过载的情况，如何值交给同进程内，一个人去做加载存储？

**使用归并回源的思路**

同进程只交给一个人去获取mysql数据，然后批量返回。同时这个lease owner投递一个kafka消息，做index cache的recovery操作。这样可以大大减少mysql的压力，以及大量透穿导致的密集写kafka的问题。

更进一步的，后续连续的请求，仍然可能会短时cache miss，我们可以在进程内设置一个short-lived flag，标记最近有一个人投递了cache rebuild的消息，直接drop。

### 热点

流量热点是因为突然热门的主题，被高频次的访问，因为底层的cache设计，一般的按照主题key进行一致性hash来进行分片，但是热点key一定命中某一个节点，这时候remote cache可能会变为瓶颈，因此做cache的升级local cache是有必要的，我们一般使用单进程自适应发现热点的思路，附加一个短时的ttl local cache，可以在进程内吞掉大量的读请求。

在内存中使用hashmap统计每个key的访问频次，这里可以使用滑动窗口统计，即每个窗口中，维护一个hashmap，之后统计所有未过去的bucket，汇总所有key的数据，之后使用小堆计算TopK的数据，自动进行热点识别。